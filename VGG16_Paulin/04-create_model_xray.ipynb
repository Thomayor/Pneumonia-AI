{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e08565b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c5ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vgg16_model():\n",
    "    conv_base = VGG16(weights='imagenet',\n",
    "                      include_top=False,\n",
    "                      input_shape=(150, 150, 3))\n",
    "    \n",
    "    for layer in conv_base.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        conv_base,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d88c5",
   "metadata": {},
   "source": [
    "## 3. Architecture du Mod√®le - VGG16 avec Fine-Tuning\n",
    "\n",
    "### üß¨ Principe du Transfer Learning\n",
    "\n",
    "Le **transfer learning** consiste √† utiliser un mod√®le pr√©-entra√Æn√© (ici VGG16) et l'adapter √† notre t√¢che sp√©cifique. Cette approche pr√©sente plusieurs avantages :\n",
    "\n",
    "- ‚ö° **Acc√©l√©ration de l'entra√Ænement** : Utilisation de features d√©j√† apprises\n",
    "- üìà **Am√©lioration des performances** : B√©n√©fice de l'expertise acquise sur ImageNet\n",
    "- üíæ **R√©duction des donn√©es n√©cessaires** : Efficace m√™me avec un dataset limit√©\n",
    "- üéØ **Meilleure g√©n√©ralisation** : Features pr√©-entra√Æn√©es robustes\n",
    "\n",
    "### üèóÔ∏è Architecture du mod√®le VGG16\n",
    "\n",
    "**VGG16** est un r√©seau de neurones convolutionnel compos√© de :\n",
    "- **13 couches convolutionnelles** avec filtres 3√ó3\n",
    "- **5 couches de pooling** pour r√©duire la dimensionnalit√©  \n",
    "- **3 couches fully connected** (exclues avec `include_top=False`)\n",
    "\n",
    "### ‚öôÔ∏è Configuration du Fine-Tuning\n",
    "\n",
    "#### Chargement du mod√®le pr√©-entra√Æn√©\n",
    "```python\n",
    "conv_base = VGG16(weights='imagenet',           # Poids pr√©-entra√Æn√©s sur ImageNet\n",
    "                  include_top=False,            # Exclusion des couches de classification\n",
    "                  input_shape=(150, 150, 3))    # Adaptation √† nos images 150√ó150√ó3\n",
    "```\n",
    "\n",
    "#### Strat√©gie de gel des couches\n",
    "```python\n",
    "for layer in conv_base.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "```\n",
    "\n",
    "**Principe du fine-tuning :**\n",
    "- üîí **Couches gel√©es** : Les premi√®res couches (features de bas niveau) restent fig√©es\n",
    "- üîì **Couches d√©gel√©es** : Les 4 derni√®res couches sont r√©entra√Æn√©es pour notre t√¢che\n",
    "- üéØ **Objectif** : Adapter les features de haut niveau aux radiographies thoraciques\n",
    "\n",
    "### üèõÔ∏è Architecture finale du mod√®le\n",
    "\n",
    "#### Couches ajout√©es pour la classification\n",
    "1. **`Flatten()`** : Conversion des feature maps 2D en vecteur 1D\n",
    "2. **`Dense(256, activation='relu')`** : Couche fully connected avec 256 neurones\n",
    "3. **`Dropout(0.5)`** : R√©gularisation pour √©viter le surapprentissage (50% dropout)\n",
    "4. **`Dense(1, activation='sigmoid')`** : Couche de sortie pour classification binaire\n",
    "\n",
    "#### Configuration de l'optimisation\n",
    "- **Loss function** : `binary_crossentropy` (adapt√© √† la classification binaire)\n",
    "- **Optimizer** : `Adam` avec learning rate faible (0.0001)\n",
    "- **M√©triques** : `accuracy` pour suivre les performances\n",
    "\n",
    "### üí° Avantages de cette architecture\n",
    "\n",
    "|           Aspect          |                       B√©n√©fice                            |\n",
    "|---------------------------|-----------------------------------------------------------|\n",
    "| **Poids pr√©-entra√Æn√©s**   | Connaissance pr√©alable des motifs visuels                 |\n",
    "| **Fine-tuning s√©lectif**  | Adaptation sp√©cifique sans perdre les features g√©n√©rales  |\n",
    "| **Dropout**               | Pr√©vention du surapprentissage                            |\n",
    "| **Learning rate faible**  | Ajustements fins sans perturber les features pr√©-apprises |\n",
    "\n",
    "> ‚ö†Ô∏è **Conseil** : Cette architecture √©quilibre performance et efficacit√© computationnelle, particuli√®rement adapt√©e √† la classification d'images m√©dicales avec des datasets de taille mod√©r√©e."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
